{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24d31280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import or_gym\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c45d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = (2023)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4267421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this assignment, you will train a reinforcement learning agent to solve the Bounded Knapsack problem with 200 items\n",
    "env = or_gym.make(\"Knapsack-v2\", max_weight=300, mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "979fa05d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "KnapsackEnv.render() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m env\u001b[39m.\u001b[39mreset() \u001b[39m# render the envrionment \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     env\u001b[39m.\u001b[39mrender()\n\u001b[1;32m      4\u001b[0m     env\u001b[39m.\u001b[39mstep(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample())\n\u001b[1;32m      5\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/envs/1BM120/lib/python3.11/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: KnapsackEnv.render() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "env.reset() # render the envrionment \n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa233909",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space\n",
    "action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b84a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.reset())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466bbc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space #Get number of actions in an environment \n",
    "state_space = env.observation_space #Get number of states in an environment \n",
    "print('Action space', action_space)\n",
    "print('State space', state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf58cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()# sample an action from the available items\n",
    "print(action)\n",
    "\n",
    "#the action samples a number larger than the possible numbers in the array?!?!?!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bebfce7",
   "metadata": {},
   "source": [
    "The step function of the environment shows the following information: four variables, which are: the new state (St+1), reward (Rt+1), a boolean stating whether the environment is terminated or done, and extra info for debugging.\n",
    "\n",
    "The \"new state\" as shown by the stepfunction is an array of 3 lists of 201 items. An action samples from the second list showing the reward of the step according to the value of the item, the weight of that item corresponds to that same index in the first list. Therefore, env.step(0) will correspond to item-value 63, of weight 9, which can fit 33 times in the knapsack, at the 34th time sampling step(0) the environment will terminate. \n",
    "\n",
    "The last list seems to correspond to the value-weight trade-off of that item. So high numbers correspond to items that are relatively high of value and low in weight and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(199)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max weight capacity:\\t{}kg\".format(env.max_weight))\n",
    "print(\"Number of items:\\t{}\".format(env.N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf8c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642bcb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []  # Experience replay memory\n",
    "        self.gamma = 0.95  # Discount rate\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_decay = 0.995  # Decay rate for exploration\n",
    "        self.epsilon_min = 0.01  # Minimum exploration rate\n",
    "\n",
    "        # Create the neural network model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_dim=state_size, activation='relu'))\n",
    "        self.model.add(Dense(24, activation='relu'))\n",
    "        self.model.add(Dense(action_size, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=0.001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    # ...\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def experience_replay(self, batch_size):\n",
    "        batch = np.random.choice(len(self.memory), batch_size)\n",
    "        for index in batch:\n",
    "            state, action, reward, next_state, done = self.memory[index]\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30573e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "    env = gym.make('knapsack-v2')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    dqn_agent = DQN(state_size, action_size)\n",
    "\n",
    "    episodes = 1000  # Number of episodes to train\n",
    "    batch_size = 32\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        done = False\n",
    "        time_step = 0\n",
    "\n",
    "        while not done:\n",
    "            action = dqn_agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward if not done else -10\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            dqn_agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            time_step += 1\n",
    "\n",
    "        if len(dqn_agent.memory) > batch_size:\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779218b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "env = or_gym.make(\"Knapsack-v2\", max_weight=300, mask=False)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "dqn_agent = DQN(state_size, action_size)\n",
    "\n",
    "episodes = 1000  # Number of episodes to train\n",
    "batch_size = 32\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    done = False\n",
    "    time_step = 0\n",
    "\n",
    "    while not done:\n",
    "        action = dqn_agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        dqn_agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        time_step += 1\n",
    "\n",
    "    if len(dqn_agent.memory) > batch_size:\n",
    "        done\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f4755e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
